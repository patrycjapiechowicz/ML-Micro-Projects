{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrycjapiechowicz/hobby-projects/blob/main/Tensorflow_classification_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-PtUqd7wcs"
      },
      "source": [
        "## Credit Card fraud detection - Classification problem\n",
        "\n",
        "\n",
        "### Data overview\n",
        "* transactions made by credit cards in September 2013 by european cardholders\n",
        "* transactions that occurred in two days\n",
        "* 492 frauds out of 284,807 transactions\n",
        "* numerical input variables which are the result of a PCA transformation. \n",
        "* 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
        "\n",
        "\n",
        "Dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0ZKR8jp7wc0"
      },
      "outputs": [],
      "source": [
        "#%load \"g:/My Drive/fraud.py\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#df = pd.read_csv(\"../../data/creditcard.csv\")\n",
        "#df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yrvK2W_8I6S",
        "outputId": "b5abc4be-2258-4b7e-fd98-d3931055875d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/creditcard.csv\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzPNizEc8xtI",
        "outputId": "7b691d9b-5713-41c1-c2ec-7abf8fca7a6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 122881 entries, 0 to 122880\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    122881 non-null  int64  \n",
            " 1   V1      122881 non-null  float64\n",
            " 2   V2      122881 non-null  float64\n",
            " 3   V3      122881 non-null  float64\n",
            " 4   V4      122881 non-null  float64\n",
            " 5   V5      122881 non-null  float64\n",
            " 6   V6      122881 non-null  float64\n",
            " 7   V7      122881 non-null  float64\n",
            " 8   V8      122881 non-null  float64\n",
            " 9   V9      122881 non-null  float64\n",
            " 10  V10     122880 non-null  float64\n",
            " 11  V11     122880 non-null  float64\n",
            " 12  V12     122880 non-null  float64\n",
            " 13  V13     122880 non-null  float64\n",
            " 14  V14     122880 non-null  float64\n",
            " 15  V15     122880 non-null  float64\n",
            " 16  V16     122880 non-null  float64\n",
            " 17  V17     122880 non-null  float64\n",
            " 18  V18     122880 non-null  float64\n",
            " 19  V19     122880 non-null  float64\n",
            " 20  V20     122880 non-null  float64\n",
            " 21  V21     122880 non-null  float64\n",
            " 22  V22     122880 non-null  float64\n",
            " 23  V23     122880 non-null  float64\n",
            " 24  V24     122880 non-null  float64\n",
            " 25  V25     122880 non-null  float64\n",
            " 26  V26     122880 non-null  float64\n",
            " 27  V27     122880 non-null  float64\n",
            " 28  V28     122880 non-null  float64\n",
            " 29  Amount  122880 non-null  float64\n",
            " 30  Class   122880 non-null  float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 29.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "RzRoGHgRBnCG",
        "outputId": "14d90711-7b76-4b99-a17c-32f46229b38b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
              "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
              "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
              "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6239cea9-1c63-4624-b6b3-83da7a99a502\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6239cea9-1c63-4624-b6b3-83da7a99a502')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6239cea9-1c63-4624-b6b3-83da7a99a502 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6239cea9-1c63-4624-b6b3-83da7a99a502');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(columns=\"Class\")\n",
        "y = df.Class\n"
      ],
      "metadata": {
        "id": "8a_x1hY4CmR-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "NUB6koMuCqNJ",
        "outputId": "4f1b39c1-8ac0-493b-9a29-a31536ca7b34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Time     V1     V2     V3     V4     V5     V6     V7     V8     V9  \\\n",
              "0       False  False  False  False  False  False  False  False  False  False   \n",
              "1       False  False  False  False  False  False  False  False  False  False   \n",
              "2       False  False  False  False  False  False  False  False  False  False   \n",
              "3       False  False  False  False  False  False  False  False  False  False   \n",
              "4       False  False  False  False  False  False  False  False  False  False   \n",
              "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "122876  False  False  False  False  False  False  False  False  False  False   \n",
              "122877  False  False  False  False  False  False  False  False  False  False   \n",
              "122878  False  False  False  False  False  False  False  False  False  False   \n",
              "122879  False  False  False  False  False  False  False  False  False  False   \n",
              "122880  False  False  False  False  False  False  False  False  False  False   \n",
              "\n",
              "        ...    V21    V22    V23    V24    V25    V26    V27    V28  Amount  \\\n",
              "0       ...  False  False  False  False  False  False  False  False   False   \n",
              "1       ...  False  False  False  False  False  False  False  False   False   \n",
              "2       ...  False  False  False  False  False  False  False  False   False   \n",
              "3       ...  False  False  False  False  False  False  False  False   False   \n",
              "4       ...  False  False  False  False  False  False  False  False   False   \n",
              "...     ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
              "122876  ...  False  False  False  False  False  False  False  False   False   \n",
              "122877  ...  False  False  False  False  False  False  False  False   False   \n",
              "122878  ...  False  False  False  False  False  False  False  False   False   \n",
              "122879  ...  False  False  False  False  False  False  False  False   False   \n",
              "122880  ...   True   True   True   True   True   True   True   True    True   \n",
              "\n",
              "        Class  \n",
              "0       False  \n",
              "1       False  \n",
              "2       False  \n",
              "3       False  \n",
              "4       False  \n",
              "...       ...  \n",
              "122876  False  \n",
              "122877  False  \n",
              "122878  False  \n",
              "122879  False  \n",
              "122880   True  \n",
              "\n",
              "[122881 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-074faad5-bda3-4fee-8cc5-5f00b194b900\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122876</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122877</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122878</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122879</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122880</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122881 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-074faad5-bda3-4fee-8cc5-5f00b194b900')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-074faad5-bda3-4fee-8cc5-5f00b194b900 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-074faad5-bda3-4fee-8cc5-5f00b194b900');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[122880]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq4OXI6KDFpd",
        "outputId": "eb3bcc27-4715-4b8c-c2e9-4eda8ec076ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      76723.000000\n",
              "V1           -1.708808\n",
              "V2            0.777698\n",
              "V3            1.059110\n",
              "V4           -0.455618\n",
              "V5            0.345712\n",
              "V6           -0.893250\n",
              "V7            1.396244\n",
              "V8           -0.381813\n",
              "V9           -0.300000\n",
              "V10                NaN\n",
              "V11                NaN\n",
              "V12                NaN\n",
              "V13                NaN\n",
              "V14                NaN\n",
              "V15                NaN\n",
              "V16                NaN\n",
              "V17                NaN\n",
              "V18                NaN\n",
              "V19                NaN\n",
              "V20                NaN\n",
              "V21                NaN\n",
              "V22                NaN\n",
              "V23                NaN\n",
              "V24                NaN\n",
              "V25                NaN\n",
              "V26                NaN\n",
              "V27                NaN\n",
              "V28                NaN\n",
              "Amount             NaN\n",
              "Class              NaN\n",
              "Name: 122880, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(index=122880)"
      ],
      "metadata": {
        "id": "cP5Ton2bDLNl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5JjssgwDDWwY",
        "outputId": "b0fcdad9-83f5-42ab-bbc2-ca2952a0c6eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Time     V1     V2     V3     V4     V5     V6     V7     V8     V9  \\\n",
              "0       False  False  False  False  False  False  False  False  False  False   \n",
              "1       False  False  False  False  False  False  False  False  False  False   \n",
              "2       False  False  False  False  False  False  False  False  False  False   \n",
              "3       False  False  False  False  False  False  False  False  False  False   \n",
              "4       False  False  False  False  False  False  False  False  False  False   \n",
              "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "122875  False  False  False  False  False  False  False  False  False  False   \n",
              "122876  False  False  False  False  False  False  False  False  False  False   \n",
              "122877  False  False  False  False  False  False  False  False  False  False   \n",
              "122878  False  False  False  False  False  False  False  False  False  False   \n",
              "122879  False  False  False  False  False  False  False  False  False  False   \n",
              "\n",
              "        ...    V21    V22    V23    V24    V25    V26    V27    V28  Amount  \\\n",
              "0       ...  False  False  False  False  False  False  False  False   False   \n",
              "1       ...  False  False  False  False  False  False  False  False   False   \n",
              "2       ...  False  False  False  False  False  False  False  False   False   \n",
              "3       ...  False  False  False  False  False  False  False  False   False   \n",
              "4       ...  False  False  False  False  False  False  False  False   False   \n",
              "...     ...    ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
              "122875  ...  False  False  False  False  False  False  False  False   False   \n",
              "122876  ...  False  False  False  False  False  False  False  False   False   \n",
              "122877  ...  False  False  False  False  False  False  False  False   False   \n",
              "122878  ...  False  False  False  False  False  False  False  False   False   \n",
              "122879  ...  False  False  False  False  False  False  False  False   False   \n",
              "\n",
              "        Class  \n",
              "0       False  \n",
              "1       False  \n",
              "2       False  \n",
              "3       False  \n",
              "4       False  \n",
              "...       ...  \n",
              "122875  False  \n",
              "122876  False  \n",
              "122877  False  \n",
              "122878  False  \n",
              "122879  False  \n",
              "\n",
              "[122880 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee538ca9-a1f3-429d-84b6-2ec301958ee8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122875</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122876</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122877</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122878</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122879</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122880 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee538ca9-a1f3-429d-84b6-2ec301958ee8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee538ca9-a1f3-429d-84b6-2ec301958ee8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee538ca9-a1f3-429d-84b6-2ec301958ee8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "scrolled": true,
        "id": "uvpjg3jv7wc3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(columns=\"Class\")\n",
        "y = df.Class\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
        "                 test_size=.5,\n",
        "                 random_state=42,stratify=y)\n",
        "\n",
        "# %load \"g:/My Drive/diamonds_preproc.py\"\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1. select labels of features for one hot encoding and standarization\n",
        "num_labels = X_train.select_dtypes(\"number\").columns\n",
        "cat_labels = X_train.select_dtypes(\"object\").columns\n",
        "\n",
        "# 2. define preprocessor for each type of data\n",
        "cat_preprocessor = OneHotEncoder(drop = \"if_binary\")\n",
        "num_preprocessor = RobustScaler()\n",
        "\n",
        "# 3. Combine it all together\n",
        "preprocessor = ColumnTransformer([(\"cat\", cat_preprocessor, cat_labels),\n",
        "                                 (\"num\", num_preprocessor, num_labels)])\n",
        "\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "#feature_labels = preprocessor.transformers_[0][1].get_feature_names_out().tolist()\n",
        "#feature_labels.extend(num_labels)\n",
        "feature_labels = num_labels\n",
        "\n",
        "X_train_prepd = preprocessor.transform(X_train)\n",
        "X_train_prepd = pd.DataFrame(X_train_prepd,columns = feature_labels)\n",
        "\n",
        "X_test_prepd = preprocessor.transform(X_test)\n",
        "X_test_prepd = pd.DataFrame(X_test_prepd,columns = feature_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZyOHG4Zg7wc7"
      },
      "outputs": [],
      "source": [
        "# Simple sequential api\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "neg, pos = np.bincount(y_train)\n",
        "total = neg + pos\n",
        "\n",
        "#gdy zbilansowany\n",
        "#neg =100\n",
        "#pos =100\n",
        "\n",
        "initial_bias = np.log(pos/neg)\n",
        "initial_bias = keras.initializers.Constant(initial_bias)\n",
        "initial_bias\n",
        "\n",
        "metrics_to_monitor = [\n",
        "    keras.metrics.AUC(name = \"auc\"),\n",
        "    keras.metrics.AUC(curve=\"PR\", name = \"prc\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "u4pCsCG57wc8"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    input_layer = layers.Input(shape = (X_train_prepd.shape[1],),name = \"input\")\n",
        "\n",
        "    h1 = layers.Dense(64,activation = \"relu\", name = \"h1\")(input_layer)\n",
        "    h2 = layers.Dense(64,activation = \"relu\", name = \"h2\")(h1)\n",
        "    h3 = layers.Dense(64,activation = \"relu\", name = \"h3\")(h2)\n",
        "\n",
        "\n",
        "    output_layer = layers.Dense(1,activation = \"sigmoid\", name = \"out\",\n",
        "                                bias_initializer=initial_bias)(h3)\n",
        "\n",
        "    model = keras.Model(input_layer, output_layer)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = metrics_to_monitor)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UxKiWLhZ7wc9"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "wagi = compute_class_weight(\"balanced\", classes=y_train.unique(),y= y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AKoPMbcX7wc-"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "logs_path = \"logs\\\\\"+datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
        "\n",
        "early = keras.callbacks.EarlyStopping(monitor = \"val_prc\",\n",
        "                                  mode = \"max\",\n",
        "                                  patience=30,\n",
        "                                  restore_best_weights=True)\n",
        "cb = [early]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2GNpKHs7wc_",
        "outputId": "e483b392-0a56-45aa-e374-b81a5638528f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "48/48 [==============================] - 2s 19ms/step - loss: 0.8296 - auc: 0.8865 - prc: 0.1486 - val_loss: 0.1129 - val_auc: 0.9514 - val_prc: 0.1029\n",
            "Epoch 2/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.3528 - auc: 0.9468 - prc: 0.1834 - val_loss: 0.0869 - val_auc: 0.9609 - val_prc: 0.2157\n",
            "Epoch 3/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.2770 - auc: 0.9559 - prc: 0.3151 - val_loss: 0.0402 - val_auc: 0.9238 - val_prc: 0.6402\n",
            "Epoch 4/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.2140 - auc: 0.9736 - prc: 0.5335 - val_loss: 0.1211 - val_auc: 0.9570 - val_prc: 0.3234\n",
            "Epoch 5/100\n",
            "48/48 [==============================] - 1s 13ms/step - loss: 0.1594 - auc: 0.9849 - prc: 0.5356 - val_loss: 0.0856 - val_auc: 0.9307 - val_prc: 0.6304\n",
            "Epoch 6/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.1195 - auc: 0.9921 - prc: 0.7345 - val_loss: 0.0777 - val_auc: 0.9242 - val_prc: 0.6872\n",
            "Epoch 7/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0868 - auc: 0.9965 - prc: 0.7548 - val_loss: 0.0412 - val_auc: 0.9270 - val_prc: 0.7545\n",
            "Epoch 8/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0923 - auc: 0.9961 - prc: 0.5412 - val_loss: 0.0710 - val_auc: 0.9296 - val_prc: 0.6564\n",
            "Epoch 9/100\n",
            "48/48 [==============================] - 1s 16ms/step - loss: 0.0639 - auc: 0.9985 - prc: 0.7616 - val_loss: 0.0613 - val_auc: 0.9154 - val_prc: 0.7183\n",
            "Epoch 10/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0519 - auc: 0.9992 - prc: 0.7874 - val_loss: 0.0456 - val_auc: 0.9176 - val_prc: 0.7178\n",
            "Epoch 11/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0433 - auc: 0.9995 - prc: 0.7946 - val_loss: 0.0492 - val_auc: 0.9205 - val_prc: 0.7181\n",
            "Epoch 12/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0365 - auc: 0.9996 - prc: 0.8105 - val_loss: 0.0405 - val_auc: 0.9241 - val_prc: 0.7180\n",
            "Epoch 13/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0476 - auc: 0.9987 - prc: 0.5218 - val_loss: 0.1189 - val_auc: 0.9242 - val_prc: 0.2298\n",
            "Epoch 14/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0495 - auc: 0.9985 - prc: 0.5429 - val_loss: 0.0349 - val_auc: 0.9162 - val_prc: 0.6568\n",
            "Epoch 15/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0332 - auc: 0.9996 - prc: 0.7894 - val_loss: 0.0416 - val_auc: 0.9280 - val_prc: 0.7183\n",
            "Epoch 16/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0269 - auc: 0.9997 - prc: 0.8134 - val_loss: 0.0257 - val_auc: 0.9166 - val_prc: 0.7182\n",
            "Epoch 17/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0223 - auc: 0.9998 - prc: 0.8270 - val_loss: 0.0234 - val_auc: 0.9186 - val_prc: 0.7183\n",
            "Epoch 18/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0202 - auc: 0.9998 - prc: 0.8283 - val_loss: 0.0190 - val_auc: 0.9221 - val_prc: 0.7187\n",
            "Epoch 19/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0188 - auc: 0.9998 - prc: 0.8354 - val_loss: 0.0220 - val_auc: 0.9195 - val_prc: 0.7182\n",
            "Epoch 20/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0160 - auc: 0.9998 - prc: 0.8417 - val_loss: 0.0221 - val_auc: 0.9223 - val_prc: 0.7187\n",
            "Epoch 21/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0145 - auc: 0.9998 - prc: 0.8462 - val_loss: 0.0191 - val_auc: 0.9224 - val_prc: 0.7185\n",
            "Epoch 22/100\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 0.0136 - auc: 0.9998 - prc: 0.8442 - val_loss: 0.0174 - val_auc: 0.9245 - val_prc: 0.7546\n",
            "Epoch 23/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0120 - auc: 0.9998 - prc: 0.8412 - val_loss: 0.0181 - val_auc: 0.9231 - val_prc: 0.7522\n",
            "Epoch 24/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0103 - auc: 0.9998 - prc: 0.8488 - val_loss: 0.0159 - val_auc: 0.9249 - val_prc: 0.7515\n",
            "Epoch 25/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0090 - auc: 0.9998 - prc: 0.8457 - val_loss: 0.0145 - val_auc: 0.9255 - val_prc: 0.7428\n",
            "Epoch 26/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0082 - auc: 0.9998 - prc: 0.8426 - val_loss: 0.0130 - val_auc: 0.9083 - val_prc: 0.7510\n",
            "Epoch 27/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0077 - auc: 0.9998 - prc: 0.8496 - val_loss: 0.0126 - val_auc: 0.9088 - val_prc: 0.7399\n",
            "Epoch 28/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0068 - auc: 0.9998 - prc: 0.8498 - val_loss: 0.0125 - val_auc: 0.9096 - val_prc: 0.7448\n",
            "Epoch 29/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0062 - auc: 0.9998 - prc: 0.8508 - val_loss: 0.0129 - val_auc: 0.9091 - val_prc: 0.7296\n",
            "Epoch 30/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0054 - auc: 0.9998 - prc: 0.8558 - val_loss: 0.0111 - val_auc: 0.8907 - val_prc: 0.7279\n",
            "Epoch 31/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0053 - auc: 0.9998 - prc: 0.8682 - val_loss: 0.0110 - val_auc: 0.8913 - val_prc: 0.7454\n",
            "Epoch 32/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0052 - auc: 0.9998 - prc: 0.8652 - val_loss: 0.0114 - val_auc: 0.8907 - val_prc: 0.7287\n",
            "Epoch 33/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0044 - auc: 0.9999 - prc: 0.9005 - val_loss: 0.0100 - val_auc: 0.8919 - val_prc: 0.7315\n",
            "Epoch 34/100\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 0.0046 - auc: 0.9999 - prc: 0.8874 - val_loss: 0.0120 - val_auc: 0.8913 - val_prc: 0.7390\n",
            "Epoch 35/100\n",
            "48/48 [==============================] - 1s 13ms/step - loss: 0.0045 - auc: 0.9999 - prc: 0.8960 - val_loss: 0.0118 - val_auc: 0.8919 - val_prc: 0.7512\n",
            "Epoch 36/100\n",
            "48/48 [==============================] - 1s 15ms/step - loss: 0.0042 - auc: 0.9999 - prc: 0.8870 - val_loss: 0.0115 - val_auc: 0.8918 - val_prc: 0.7368\n",
            "Epoch 37/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0037 - auc: 0.9999 - prc: 0.9202 - val_loss: 0.0123 - val_auc: 0.8918 - val_prc: 0.7424\n",
            "Epoch 38/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0034 - auc: 0.9999 - prc: 0.9230 - val_loss: 0.0108 - val_auc: 0.8924 - val_prc: 0.7330\n",
            "Epoch 39/100\n",
            "48/48 [==============================] - 1s 13ms/step - loss: 0.0031 - auc: 0.9999 - prc: 0.9263 - val_loss: 0.0100 - val_auc: 0.8926 - val_prc: 0.7328\n",
            "Epoch 40/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0069 - auc: 0.9999 - prc: 0.8917 - val_loss: 0.0156 - val_auc: 0.8926 - val_prc: 0.7175\n",
            "Epoch 41/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0095 - auc: 0.9998 - prc: 0.8381 - val_loss: 0.0144 - val_auc: 0.8927 - val_prc: 0.7175\n",
            "Epoch 42/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0074 - auc: 0.9998 - prc: 0.8498 - val_loss: 0.0129 - val_auc: 0.8928 - val_prc: 0.7175\n",
            "Epoch 43/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0059 - auc: 0.9998 - prc: 0.8602 - val_loss: 0.0107 - val_auc: 0.8935 - val_prc: 0.7514\n",
            "Epoch 44/100\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.0046 - auc: 0.9998 - prc: 0.8591 - val_loss: 0.0107 - val_auc: 0.8931 - val_prc: 0.7378\n",
            "Epoch 45/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0035 - auc: 0.9998 - prc: 0.8579 - val_loss: 0.0090 - val_auc: 0.8939 - val_prc: 0.7299\n",
            "Epoch 46/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0062 - auc: 0.9997 - prc: 0.7731 - val_loss: 0.0099 - val_auc: 0.8939 - val_prc: 0.7419\n",
            "Epoch 47/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0041 - auc: 0.9998 - prc: 0.8565 - val_loss: 0.0133 - val_auc: 0.8918 - val_prc: 0.7216\n",
            "Epoch 48/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0034 - auc: 0.9998 - prc: 0.8559 - val_loss: 0.0086 - val_auc: 0.8940 - val_prc: 0.7216\n",
            "Epoch 49/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0028 - auc: 0.9999 - prc: 0.9100 - val_loss: 0.0101 - val_auc: 0.8933 - val_prc: 0.7308\n",
            "Epoch 50/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0025 - auc: 0.9999 - prc: 0.8835 - val_loss: 0.0095 - val_auc: 0.8940 - val_prc: 0.7320\n",
            "Epoch 51/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0021 - auc: 0.9999 - prc: 0.9297 - val_loss: 0.0088 - val_auc: 0.8942 - val_prc: 0.7628\n",
            "Epoch 52/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0023 - auc: 0.9999 - prc: 0.8848 - val_loss: 0.0090 - val_auc: 0.8943 - val_prc: 0.7347\n",
            "Epoch 53/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0018 - auc: 0.9999 - prc: 0.9491 - val_loss: 0.0087 - val_auc: 0.8946 - val_prc: 0.7362\n",
            "Epoch 54/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0018 - auc: 0.9999 - prc: 0.9389 - val_loss: 0.0099 - val_auc: 0.8943 - val_prc: 0.7437\n",
            "Epoch 55/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0019 - auc: 0.9999 - prc: 0.8961 - val_loss: 0.0092 - val_auc: 0.8945 - val_prc: 0.7341\n",
            "Epoch 56/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0019 - auc: 0.9999 - prc: 0.9136 - val_loss: 0.0093 - val_auc: 0.8947 - val_prc: 0.7403\n",
            "Epoch 57/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0038 - auc: 0.9999 - prc: 0.8995 - val_loss: 0.0130 - val_auc: 0.8947 - val_prc: 0.7178\n",
            "Epoch 58/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0050 - auc: 0.9998 - prc: 0.8536 - val_loss: 0.0110 - val_auc: 0.8947 - val_prc: 0.7178\n",
            "Epoch 59/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0027 - auc: 0.9999 - prc: 0.8796 - val_loss: 0.0096 - val_auc: 0.8947 - val_prc: 0.7458\n",
            "Epoch 60/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0024 - auc: 0.9999 - prc: 0.9070 - val_loss: 0.0129 - val_auc: 0.8944 - val_prc: 0.7177\n",
            "Epoch 61/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0056 - auc: 0.9998 - prc: 0.8597 - val_loss: 0.0109 - val_auc: 0.8949 - val_prc: 0.7179\n",
            "Epoch 62/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0040 - auc: 0.9998 - prc: 0.8662 - val_loss: 0.0101 - val_auc: 0.8949 - val_prc: 0.7095\n",
            "Epoch 63/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0029 - auc: 0.9998 - prc: 0.8650 - val_loss: 0.0093 - val_auc: 0.8949 - val_prc: 0.7333\n",
            "Epoch 64/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0022 - auc: 0.9998 - prc: 0.8650 - val_loss: 0.0091 - val_auc: 0.8949 - val_prc: 0.7196\n",
            "Epoch 65/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0019 - auc: 0.9999 - prc: 0.8956 - val_loss: 0.0097 - val_auc: 0.8949 - val_prc: 0.7404\n",
            "Epoch 66/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0016 - auc: 0.9999 - prc: 0.9159 - val_loss: 0.0091 - val_auc: 0.8949 - val_prc: 0.7634\n",
            "Epoch 67/100\n",
            "48/48 [==============================] - 1s 15ms/step - loss: 0.0013 - auc: 1.0000 - prc: 0.9698 - val_loss: 0.0095 - val_auc: 0.8949 - val_prc: 0.7319\n",
            "Epoch 68/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0014 - auc: 0.9999 - prc: 0.9250 - val_loss: 0.0095 - val_auc: 0.8949 - val_prc: 0.7602\n",
            "Epoch 69/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0010 - auc: 1.0000 - prc: 0.9898 - val_loss: 0.0095 - val_auc: 0.8951 - val_prc: 0.7429\n",
            "Epoch 70/100\n",
            "48/48 [==============================] - 1s 16ms/step - loss: 0.0014 - auc: 0.9999 - prc: 0.9072 - val_loss: 0.0101 - val_auc: 0.8950 - val_prc: 0.7480\n",
            "Epoch 71/100\n",
            "48/48 [==============================] - 1s 15ms/step - loss: 0.0011 - auc: 0.9999 - prc: 0.9340 - val_loss: 0.0087 - val_auc: 0.8951 - val_prc: 0.7760\n",
            "Epoch 72/100\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 0.0011 - auc: 1.0000 - prc: 0.9792 - val_loss: 0.0090 - val_auc: 0.8951 - val_prc: 0.7832\n",
            "Epoch 73/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 9.3583e-04 - auc: 1.0000 - prc: 0.9612 - val_loss: 0.0092 - val_auc: 0.8951 - val_prc: 0.7798\n",
            "Epoch 74/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 7.8305e-04 - auc: 1.0000 - prc: 0.9897 - val_loss: 0.0084 - val_auc: 0.8952 - val_prc: 0.7802\n",
            "Epoch 75/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0010 - auc: 1.0000 - prc: 0.9993 - val_loss: 0.0108 - val_auc: 0.8950 - val_prc: 0.6803\n",
            "Epoch 76/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.0036 - auc: 0.9999 - prc: 0.8772 - val_loss: 0.0096 - val_auc: 0.8952 - val_prc: 0.7087\n",
            "Epoch 77/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0013 - auc: 0.9999 - prc: 0.9412 - val_loss: 0.0093 - val_auc: 0.8951 - val_prc: 0.7717\n",
            "Epoch 78/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 7.3703e-04 - auc: 1.0000 - prc: 0.9897 - val_loss: 0.0092 - val_auc: 0.8951 - val_prc: 0.7717\n",
            "Epoch 79/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 7.8753e-04 - auc: 1.0000 - prc: 0.9804 - val_loss: 0.0088 - val_auc: 0.8952 - val_prc: 0.7814\n",
            "Epoch 80/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 9.6370e-04 - auc: 1.0000 - prc: 0.9801 - val_loss: 0.0108 - val_auc: 0.8951 - val_prc: 0.6804\n",
            "Epoch 81/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0012 - auc: 0.9999 - prc: 0.9167 - val_loss: 0.0092 - val_auc: 0.8951 - val_prc: 0.7239\n",
            "Epoch 82/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 7.9006e-04 - auc: 1.0000 - prc: 0.9800 - val_loss: 0.0095 - val_auc: 0.8952 - val_prc: 0.7421\n",
            "Epoch 83/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 5.7310e-04 - auc: 1.0000 - prc: 0.9999 - val_loss: 0.0092 - val_auc: 0.8952 - val_prc: 0.7770\n",
            "Epoch 84/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 5.7885e-04 - auc: 1.0000 - prc: 0.9804 - val_loss: 0.0094 - val_auc: 0.8953 - val_prc: 0.7815\n",
            "Epoch 85/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0015 - auc: 0.9999 - prc: 0.9427 - val_loss: 0.0139 - val_auc: 0.8951 - val_prc: 0.6854\n",
            "Epoch 86/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0034 - auc: 0.9998 - prc: 0.8609 - val_loss: 0.0105 - val_auc: 0.8952 - val_prc: 0.6854\n",
            "Epoch 87/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0012 - auc: 0.9999 - prc: 0.9420 - val_loss: 0.0108 - val_auc: 0.8951 - val_prc: 0.6854\n",
            "Epoch 88/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0016 - auc: 0.9999 - prc: 0.9163 - val_loss: 0.0130 - val_auc: 0.8951 - val_prc: 0.6854\n",
            "Epoch 89/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0021 - auc: 0.9999 - prc: 0.9166 - val_loss: 0.0091 - val_auc: 0.8953 - val_prc: 0.7377\n",
            "Epoch 90/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.0023 - auc: 0.9999 - prc: 0.8750 - val_loss: 0.0102 - val_auc: 0.8951 - val_prc: 0.7273\n",
            "Epoch 91/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 5.3163e-04 - auc: 1.0000 - prc: 0.9901 - val_loss: 0.0091 - val_auc: 0.8953 - val_prc: 0.7375\n",
            "Epoch 92/100\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 4.6779e-04 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0099 - val_auc: 0.8952 - val_prc: 0.7396\n",
            "Epoch 93/100\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 4.6941e-04 - auc: 1.0000 - prc: 0.9998 - val_loss: 0.0095 - val_auc: 0.8952 - val_prc: 0.7359\n",
            "Epoch 94/100\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 3.6201e-04 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0097 - val_auc: 0.8953 - val_prc: 0.7421\n",
            "Epoch 95/100\n",
            "48/48 [==============================] - 1s 12ms/step - loss: 4.9909e-04 - auc: 1.0000 - prc: 0.9802 - val_loss: 0.0097 - val_auc: 0.8952 - val_prc: 0.7399\n",
            "Epoch 96/100\n",
            "48/48 [==============================] - 1s 14ms/step - loss: 0.0032 - auc: 0.9999 - prc: 0.9257 - val_loss: 0.0123 - val_auc: 0.8953 - val_prc: 0.7180\n",
            "Epoch 97/100\n",
            "48/48 [==============================] - 1s 15ms/step - loss: 0.0036 - auc: 0.9999 - prc: 0.8761 - val_loss: 0.0122 - val_auc: 0.8950 - val_prc: 0.6626\n",
            "Epoch 98/100\n",
            "48/48 [==============================] - 1s 16ms/step - loss: 0.0223 - auc: 0.9994 - prc: 0.6630 - val_loss: 0.1188 - val_auc: 0.9289 - val_prc: 0.1693\n",
            "Epoch 99/100\n",
            "48/48 [==============================] - 1s 17ms/step - loss: 0.2804 - auc: 0.9818 - prc: 0.3224 - val_loss: 0.2236 - val_auc: 0.9380 - val_prc: 0.0994\n",
            "Epoch 100/100\n",
            "48/48 [==============================] - 1s 17ms/step - loss: 0.1990 - auc: 0.9839 - prc: 0.1004 - val_loss: 0.0685 - val_auc: 0.9473 - val_prc: 0.3317\n"
          ]
        }
      ],
      "source": [
        "model = model_init()\n",
        "\n",
        "history = model.fit(X_train_prepd,\n",
        "          y_train,\n",
        "         batch_size=1024,\n",
        "          epochs=100,\n",
        "          validation_split=.2,\n",
        "                   callbacks=cb,\n",
        "                   verbose = 1,\n",
        "                   class_weight={0: wagi[0],\n",
        "                                1: wagi[1]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDumYPr7wdB"
      },
      "source": [
        "## Tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16MXLK5t7wdC",
        "outputId": "a0148ba8-1c2f-4249-ef31-2a4cfb658550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.2.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.25.1)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.9.2)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.30.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.51.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.12)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.9.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.38.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (2.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.2.0 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hEy70447wdD",
        "outputId": "5f6e8543-f1fe-4a3c-e5b4-faae9b093934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-b62ee0137ba6>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import Objective\n"
          ]
        }
      ],
      "source": [
        "from kerastuner import Objective\n",
        "from kerastuner import RandomSearch, BayesianOptimization, Hyperband\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "hp = HyperParameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZAFOFQc7wdD",
        "outputId": "301e67dc-0a3a-40d8-d4b2-a76c0704162c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 32,  64,  96, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "np.arange(32,129,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3rYb6eFp7wdF"
      },
      "outputs": [],
      "source": [
        "max_trial = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BNZ9BGyh7wdF"
      },
      "outputs": [],
      "source": [
        "early = keras.callbacks.EarlyStopping(monitor = \"val_prc\",\n",
        "                                  mode = \"max\",\n",
        "                                  patience=30,\n",
        "                                  restore_best_weights=True)\n",
        "cb = [early]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ySZ3vKwN7wdG"
      },
      "outputs": [],
      "source": [
        "def model_init(hp):\n",
        "    input_layer = layers.Input(shape = (X_train_prepd.shape[1],),name = \"input\")\n",
        "\n",
        "    h = layers.Dense(hp.Int(\"h0_units\",32,128,32),\n",
        "                      activation = hp.Choice(\"h0_act\",[\"relu\",\"tanh\",\"selu\"]),\n",
        "                      name = \"h_0\")(input_layer)\n",
        "    \n",
        "    for i in  range(hp.Int(\"n_layers\",1,3)):\n",
        "        h = layers.Dense(hp.Int(f\"h{i}_units\",32,128,32),\n",
        "                      activation = hp.Choice(f\"h{i}_act\",[\"relu\",\"tanh\",\"selu\"]),\n",
        "                      name = f\"h{i}\")(h)\n",
        "\n",
        "\n",
        "\n",
        "    output_layer = layers.Dense(1,activation = \"sigmoid\", name = \"out\",\n",
        "                                bias_initializer=initial_bias)(h)\n",
        "\n",
        "    model = keras.Model(input_layer, output_layer)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = metrics_to_monitor)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3KoUL5xp7wdI"
      },
      "outputs": [],
      "source": [
        "tuner = RandomSearch(model_init, \n",
        "                     objective=Objective(\"val_prc\", direction=\"max\"),\n",
        "                     max_trials=5,\n",
        "                    overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBd333T07wdK",
        "outputId": "15f9c3d1-0db6-4057-9b64-1671e24337dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 00m 42s]\n",
            "val_prc: 0.8081595301628113\n",
            "\n",
            "Best val_prc So Far: 0.8081595301628113\n",
            "Total elapsed time: 00h 03m 05s\n"
          ]
        }
      ],
      "source": [
        "tuner.search(X_train_prepd,\n",
        "          y_train,\n",
        "         batch_size=1024,\n",
        "          epochs=100,\n",
        "          validation_split=.2,\n",
        "                   callbacks=cb,\n",
        "            verbose = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuJKYb2S7wdL",
        "outputId": "c56dd4c4-5622-40e2-cea1-add2e7f28e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f193813e580>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "h0_units: 32\n",
            "h0_act: relu\n",
            "n_layers: 3\n",
            "h1_units: 64\n",
            "h1_act: relu\n",
            "h2_units: 32\n",
            "h2_act: relu\n",
            "Score: 0.8081595301628113\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "h0_units: 32\n",
            "h0_act: relu\n",
            "n_layers: 1\n",
            "h1_units: 96\n",
            "h1_act: relu\n",
            "Score: 0.8043986558914185\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "h0_units: 128\n",
            "h0_act: tanh\n",
            "n_layers: 2\n",
            "h1_units: 128\n",
            "h1_act: selu\n",
            "Score: 0.8034929633140564\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "h0_units: 32\n",
            "h0_act: selu\n",
            "n_layers: 1\n",
            "h1_units: 32\n",
            "h1_act: relu\n",
            "Score: 0.8030645251274109\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "h0_units: 64\n",
            "h0_act: relu\n",
            "n_layers: 2\n",
            "h1_units: 32\n",
            "h1_act: relu\n",
            "Score: 0.7916012406349182\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Or7Yg9Yh7wdN"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZKsqkv87wdN",
        "outputId": "8bb9ddba-9442-41df-846b-ae3b6b5c7912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = best_model.predict(X_test_prepd,batch_size=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cpPRTUPt7wdO"
      },
      "outputs": [],
      "source": [
        "# %load \"g:/My Drive/roc_prc.py\"\n",
        "def plot_roc(y_test, prob, model_name = \"\"):\n",
        "    \n",
        "    from sklearn.metrics import roc_curve, roc_auc_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    fpr, recall, tresh = roc_curve(y_test, prob)\n",
        "    auc = roc_auc_score(y_test, prob)\n",
        "    \n",
        "    plt.plot(fpr, recall, label = \"{} AUC = {}\".format(model_name, auc.round(2)))\n",
        "    plt.plot([0,1], [0,1], 'r--')\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"Recall\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    \n",
        "def plot_prc(y_test, prob, model_name = \"\"):\n",
        "    \n",
        "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    precision, recall, tresh = precision_recall_curve(y_test, prob)\n",
        "    auc = average_precision_score(y_test, prob)\n",
        "    \n",
        "    plt.plot(precision, recall, label = \"{} AUPRC = {}\".format(model_name, auc.round(2)))\n",
        "    plt.xlabel(\"Precision\")\n",
        "    plt.ylabel(\"Recall\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "V4ALQiPC7wdQ",
        "outputId": "666ce485-5070-4c38-9938-9bcacaee945f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_prc(y_test,y_test_pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}